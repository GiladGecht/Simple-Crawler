{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standart Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from dateutil import parser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib3 import request\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crawler(forum_name,page_number,num_of_pages):\n",
    "    \n",
    "    http = urllib3.PoolManager() \n",
    "    page_num = str(page_number) # get current page number of forum\n",
    "    url = 'https://www.beyondblue.org.au/get-support/online-forums/' + forum_name + '/page/' + page_num \n",
    "    \n",
    "    response = http.request('GET',url) # get desired page\n",
    "    my_soup = soup(response.data,'lxml') # Create BeautifulSoup object\n",
    "    \n",
    "    Post_Count = my_soup.findAll(\"div\",attrs={\"class\":\"postsCount\"}) # Find all divs which contain the number of comments for each post\n",
    "    anchors=my_soup.findAll(\"a\",attrs={\"class\":\"sfforumThreadTitle\"}) # Find all the posts's titles\n",
    "    \n",
    "    # Create a dictionary to contain the current posts titles and number of comments\n",
    "    BeyondBlue = {\"Post_Title\":[],\n",
    "                 \"Num_of_Comments\":[],\n",
    "                 \"Url\":[]}\n",
    "    \n",
    "    # Appending the dict\n",
    "    for a in anchors:\n",
    "        BeyondBlue[\"Post_Title\"].append(a.text.encode(\"ascii\",\"ignore\"))\n",
    "        BeyondBlue[\"Url\"].append(a['href'])\n",
    "    for number in Post_Count:\n",
    "        BeyondBlue[\"Num_of_Comments\"].append(int(number.text.strip().encode(\"ascii\",\"ignore\")))\n",
    "    \n",
    "    # Create a pandas DataFrame to contain the information\n",
    "    BeyondBlue = pd.DataFrame(data=BeyondBlue)\n",
    "    \n",
    "    # Simple text cleaning to replace all dots and slashes to provide easy acccess to the urls\n",
    "    BeyondBlue['Url'] = BeyondBlue[\"Url\"].apply(lambda x: x.replace(\".\",\"\"))\n",
    "    BeyondBlue['Url'] = BeyondBlue[\"Url\"].apply(lambda x: x.replace(\"/\",\"\"))\n",
    "    \n",
    "    # list to contain all the urls\n",
    "    url_list = np.asarray(BeyondBlue['Url'])\n",
    "    \n",
    "    # Iterating through each post's url, we scrape the User's IGN, the date he posted and post's body \n",
    "    User_ID = []\n",
    "    Date = []\n",
    "    User_Post = []\n",
    "    for i in range(len(url_list)):\n",
    "        curr_url = url_list[i] # Get current post's url\n",
    "        http = urllib3.PoolManager() \n",
    "        url = 'https://www.beyondblue.org.au/get-support/online-forums/depression/' + curr_url\n",
    "        response = http.request('GET',url)\n",
    "        my_soup = soup(response.data,'lxml') # create a soup object of the current post\n",
    "        \n",
    "        UserID = my_soup.findAll(\"span\",attrs={\"class\":\"sfforumUser\"}) # Find all the user id's in the current post\n",
    "        Date_Count = my_soup.findAll(\"div\",attrs={\"class\":\"sfforumPostAge\"}) # find the date in the current post\n",
    "        User_Post.append(my_soup.findAll(\"div\",attrs={\"class\":\"sfContentBlock\"})[2].text.encode(\"ascii\",\"ignore\")) # find all the body's text\n",
    "        \n",
    "        User_ID.append(UserID[0].text.encode(\"ascii\",\"ignore\")) \n",
    "        Date.append(Date_Count[0].text.strip().encode(\"ascii\",\"ignore\"))\n",
    "        \n",
    "        print(\"Finished with {} out of {}\".format(page_number*50+i+1,len(url_list)*num_of_pages)) # tracking progress\n",
    "        \n",
    "    User_ID = np.asarray(User_ID) \n",
    "    Date = np.asarray(Date) \n",
    "    User_Post = np.asarray(User_Post) # Convert to numpy arrays to later merge\n",
    "    \n",
    "    Post_Data = np.column_stack((User_ID,Date,User_Post)) # Stack the columns along the axes to create the dataframe\n",
    "    \n",
    "    Post_Data = pd.DataFrame(data=Post_Data,columns=['User_ID','Date','User_Post']) \n",
    "    BeyondBlueCopy = BeyondBlue.copy()\n",
    "    BeyondBlueCopy = pd.concat([BeyondBlueCopy,Post_Data],axis=1,sort=False) # Concatinating the two dataframes to a single dataframe containing all the information\n",
    "    BeyondBlueCopy = BeyondBlueCopy[['User_ID','Post_Title','User_Post','Num_of_Comments','Date','Url']] # re-order the dataframe's columns\n",
    "    BeyondBlueCopy['Date'] = BeyondBlueCopy['Date'].apply(lambda x: parser.parse(x)) # convert date to a numeric representation\n",
    "    \n",
    "    return BeyondBlueCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapeBeyondBlue(forum_name,num_of_pages):\n",
    "    BeyondBlue = pd.DataFrame() # Create dataframe to hold the returned dataframed and concat it with the current one\n",
    "    for i in range(num_of_pages): # iterating through the number of pages desired\n",
    "        BeyondBlue = pd.concat([BeyondBlue,Crawler(forum_name=forum_name,page_number=i,num_of_pages=num_of_pages)],\n",
    "                              axis=0,sort=False) # call crawler function to get each page's data\n",
    "        print(\"Finished with page num:{}\".format(i+1)) # track progress\n",
    "    return(BeyondBlue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeyondBlue = BeyondBlue.reset_index().drop('index',axis=1) # re-order indices\n",
    "BeyondBlue.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
